# nanoGPT

A minimalist implementation of a GPT-style transformer model built from scratch using PyTorch.

This project demonstrates the core components of transformer architecture, including multi-head self-attention, positional embeddings, and causal masking, trained on a small custom dataset for simple conversational tasks.
Designed for educational purposes and quick experimentation with language modeling fundamentals.
