# nanoGPT

A minimalist implementation of a GPT-style transformer model built from scratch using PyTorch.

This project demonstrates the core components of transformer architecture, including multi-head self-attention, positional embeddings, and causal masking, trained on a small custom dataset for simple conversational tasks.
Designed for educational purposes and quick experimentation with language modeling fundamentals.

![image](https://github.com/user-attachments/assets/701bed97-82a3-4314-a2ba-157397ecfe9e)

